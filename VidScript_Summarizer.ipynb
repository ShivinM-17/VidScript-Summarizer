{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivinM-17/VidScript-Summarizer/blob/main/VidScript_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VHas6tobHUD"
      },
      "source": [
        "# VidScript Summarizer\n",
        "\n",
        "The primary aim of this project is to leverage cutting-edge Automatic Speech Recognition (ASR) models, integrating them into a system capable of seamlessly handling both music MP3 files and YouTube links. Through the application of these advanced ASR technologies, the project aims to generate precise and contextually relevant transcriptions for the provided musical content. The goal is to establish a comprehensive and efficient framework that accurately captures the intricacies of the audio, ensuring a thorough and meaningful representation in textual form."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Transcript - whisperx\n",
        "Summarizer - 2 hugging face model"
      ],
      "metadata": {
        "id": "v6XMF__c1-jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciQ6KcgfbrFU"
      },
      "source": [
        "### Download necessary modules and repositories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM6PrvdPaFez",
        "outputId": "be4edf23-57d5-4fee-ba42-5d627c93dd95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.6/630.6 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for faster-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# download the whisper model and the transformers library\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q transformers\n",
        "!pip install -q git+https://github.com/ShivinM-17/whisperx.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx7W4H4NpBmo",
        "outputId": "6b2e2f77-3f5a-40e4-e46d-1592e011cb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Download youtube-related libraries\n",
        "!pip install -q pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w8KTuYGLEwo",
        "outputId": "a1f55eab-a3f6-40e0-d20b-5ad6782d5f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Download libraries for text summarization\n",
        "!pip install -q torch\n",
        "!pip install -q pysbd\n",
        "!pip install -q spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIXWfXSZ2W4u",
        "outputId": "6195d5f7-3478-4720-95d9-f3c9edbe004a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/63.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q assemblyai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2iuJ32ht38d",
        "outputId": "e49f9ff2-a66c-43e2-9e15-da539e29ac10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m971.3/971.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install -q streamlit-scrollable-textbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JLt15WSES14"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Set the logging level to ignore INFO messages for specific modules\n",
        "logging.getLogger(\"pytorch_lightning.utilities.migration.utils\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"pyannote.audio\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77XYakeXks67"
      },
      "source": [
        "### Making functions for the backend of the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMmDGUeI8trh"
      },
      "source": [
        "#### Functions involving the youtube videos\n",
        "\n",
        "These consists of the functions given below:\n",
        "* `convert_date_format` : convert the dates in datetime datatype into the format - `day month_name, year`\n",
        "* `download_yt_audio` : downloads the youtube audio file, making and returning a specific path for the audio file\n",
        "* `get_vid_metadata` : takes in the YT URL, checks if its legit, then downloads the audio file and returns its metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NARmQ-ikcJy0",
        "outputId": "4386fbb0-6efd-46bf-9620-4937f84b3b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing yt_extract.py\n"
          ]
        }
      ],
      "source": [
        "# for youtube audio extraction\n",
        "%%writefile yt_extract.py\n",
        "from pathlib import Path\n",
        "import pytube\n",
        "from pytube import YouTube\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3IcmOYP7LS3",
        "outputId": "ae7026e9-dab5-4ea8-b6a4-815db46705ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_extract.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_extract.py\n",
        "# Make a function to convert date data into a specific format\n",
        "def convert_date_format(date_data:datetime):\n",
        "    try:\n",
        "        # Get day, month, year from the object\n",
        "        day = date_data.day\n",
        "        month_name = date_data.strftime('%B')\n",
        "        year = date_data.year\n",
        "\n",
        "        # Format the date as desired\n",
        "        formatted_date = f\"{day} {month_name}, {year}\"\n",
        "        return formatted_date\n",
        "\n",
        "    except ValueError as e:\n",
        "        return date_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILV1-ewI2d8B",
        "outputId": "55cb21a2-f76e-4e51-fe0c-c11bd68ab276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_extract.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_extract.py\n",
        "\n",
        "# Making a function to download the youtube audio as audio file\n",
        "def download_yt_audio(yt_obj:pytube.__main__.YouTube):\n",
        "  # Get the webm type audio (since this has good audio quality than mp3)\n",
        "  stream = yt_obj.streams.filter(only_audio=True, mime_type=\"audio/webm\").order_by('abr').desc().first()\n",
        "\n",
        "  # Create a directory to save the audio file at\n",
        "  PATH_DIR = Path('audio_files')\n",
        "\n",
        "  # Make this directory, if not exists\n",
        "  PATH_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # Create a name for the audio_file path\n",
        "  FILE_NAME = \"audio_\" + str(yt_obj.video_id) + \"_\" + str(yt_obj.author) + \".webm\"\n",
        "\n",
        "  # Now, download the file into the path\n",
        "  stream.download(output_path = PATH_DIR, filename = FILE_NAME)\n",
        "  return str(PATH_DIR / FILE_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZa2Wa5IDBYg",
        "outputId": "eef76358-1231-4ae4-f4ca-7af4fa89aa28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_extract.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_extract.py\n",
        "\n",
        "# Making a function to take only those information which is available (not [] or None)\n",
        "def categorize_info(information):\n",
        "  final_info = {}\n",
        "  for info_name, info_value in information.items():\n",
        "      if not (info_value is None or (isinstance(info_value, list) and not info_value)):\n",
        "        final_info[info_name] = info_value\n",
        "\n",
        "  return final_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuQhoso1qbdF",
        "outputId": "db22e7e6-051b-4cc8-9213-3bf1416a1870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_extract.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_extract.py\n",
        "\n",
        "# Make a function to get yt video metadata (authors, video_title, etc.)\n",
        "def get_vid_metadata(video_url:str):\n",
        "  # First get the data using YouTube module\n",
        "  yt = YouTube(video_url)\n",
        "\n",
        "  # Check if the video is available or not\n",
        "  try:\n",
        "    yt.check_availability()\n",
        "\n",
        "    # Get the duration of the video\n",
        "    duration = yt.length\n",
        "    formatted_duration = f\"{duration//60} mins, {duration%60} secs\"\n",
        "\n",
        "    # Now get all the required information of the video\n",
        "    information = {\n",
        "    'Channel Name': yt.author,\n",
        "    'Video title': yt.title,\n",
        "    'Video Id': yt.video_id,\n",
        "    'Duration of video': formatted_duration,\n",
        "    'Video Link': video_url,\n",
        "    'Channel Link': yt.channel_url,\n",
        "    'Video Description': yt.description,\n",
        "    'Video Keywords': yt.keywords,\n",
        "    'Publish Date': convert_date_format(yt.publish_date),\n",
        "    'Average Ratings': yt.rating,\n",
        "    'thumbnail_url': yt.thumbnail_url,\n",
        "    'Total Views': yt.views\n",
        "    }\n",
        "\n",
        "    # Choose only that information which is present\n",
        "    valid_info = categorize_info(information)\n",
        "\n",
        "    # Now, download the youtube video as an audio file\n",
        "    file_path = download_yt_audio(yt)\n",
        "    valid_info[\"Filepath\"] = file_path\n",
        "\n",
        "    return valid_info\n",
        "\n",
        "  except Exception as e:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHPy8l5w9wi4"
      },
      "source": [
        "#### Functions involving the transcription process\n",
        "\n",
        "These functions consist of the following:\n",
        "* `get_transcriptions_text` - returns the overall text from the sentence level results of the transcription model\n",
        "* `perform_trans`: returns the transcription of the inputted audio using WhisperX model\n",
        "* `get_transcript`: overall functionality function that takes in the YT URL, downloads and retrieves video metadata using **get_vid_metadata** function, then transcribes the audio file using the **perform_trans** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UW8CN0kLvGk",
        "outputId": "f79f3058-1199-4f32-b20a-eb0c930cd798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing yt_vid_transcribe.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile yt_vid_transcribe.py\n",
        "# For audio file transcription\n",
        "import torch\n",
        "import whisperx\n",
        "import whisperx.asr\n",
        "import textwrap\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from yt_extract import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC7h33b5ky1N",
        "outputId": "06d1a494-8be9-42fe-bc83-0a9d92ac36e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_vid_transcribe.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_vid_transcribe.py\n",
        "\n",
        "# Function to get only the text data from the sentence level data results\n",
        "def get_transcriptions_text(result_data):\n",
        "  resultant_text = \"\"\n",
        "  for data in result_data[\"segments\"]:\n",
        "    resultant_text += data['text'] + \" \"\n",
        "  return resultant_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmHQslsvgEMV",
        "outputId": "88f9a51d-3387-41b0-93fc-7d6502797c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_vid_transcribe.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_vid_transcribe.py\n",
        "\n",
        "# Function to perform the overall transcription of the audio file\n",
        "def perform_trans(audio_file:str,\n",
        "                  transcript_model:whisperx.asr.FasterWhisperPipeline,\n",
        "                  align_model_pack:tuple = None,\n",
        "                  device:str='cuda', batch_size:int=16):\n",
        "\n",
        "  # Start the timer\n",
        "  start = time.time()\n",
        "\n",
        "  # Load the audio file into the model\n",
        "  audio = whisperx.load_audio(audio_file)\n",
        "\n",
        "  # Get sentence level results\n",
        "  sent_lvl_results = transcript_model.transcribe(audio,\n",
        "                                                 batch_size=batch_size)\n",
        "                                                #  print_progress = True)\n",
        "  # End the timer\n",
        "  end = time.time()\n",
        "  print()\n",
        "  print(f\"Total time to transcript: {end-start}\\n\")\n",
        "\n",
        "  # Setup a flag if we are getting word level results or not\n",
        "  flag = False\n",
        "\n",
        "  # Check if the aligner model pack is inputted or not\n",
        "  # this is used to get word-level-results\n",
        "  if align_model_pack is not None:\n",
        "    align_model, metadata = align_model_pack\n",
        "    # word level results\n",
        "    word_lvl_results = whisperx.align(sent_lvl_results[\"segments\"], align_model,\n",
        "                                      metadata, audio, device,\n",
        "                                      return_char_alignments=False)\n",
        "    flag = True\n",
        "\n",
        "  # Get just the text data from the sentence level results\n",
        "  rts_text = get_transcriptions_text(sent_lvl_results)\n",
        "\n",
        "  if flag:\n",
        "    return {'sentence_lvl_results': sent_lvl_results,\n",
        "            'word_lvl_results': word_lvl_results,\n",
        "            'text_data': rts_text}, (end-start)\n",
        "\n",
        "  return {'sentence_lvl_results': sent_lvl_results,\n",
        "          'text_data': rts_text}, (end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nO0ruw6-u0h",
        "outputId": "4ed0fb63-84cb-41ec-a4ee-b231fc233cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_vid_transcribe.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_vid_transcribe.py\n",
        "\n",
        "# Making a function that takes in the YT audio and transcribes it using the model\n",
        "# Just like a overall functionality function\n",
        "\n",
        "def get_transcript(video_data, audio_file_path):\n",
        "\n",
        "  # If the link is valid, then transcribe it using its path\n",
        "  # First, load the model\n",
        "  model = whisperx.load_model(\"medium\", device=\"cuda\", compute_type=\"float16\", language=\"en\")\n",
        "\n",
        "  # Now, get the audio file_path and use the model to transcribe the file\n",
        "  transcript_results, time_taken = perform_trans(audio_file = audio_file_path,\n",
        "                                     transcript_model = model,\n",
        "                                     device='cuda', batch_size=16)\n",
        "\n",
        "  video_data['sentence_level_text'] = transcript_results['sentence_lvl_results']\n",
        "  video_data['transcribed_text'] = transcript_results['text_data']\n",
        "  video_data['trct_time'] = time_taken\n",
        "\n",
        "  return video_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchuWWWKJp8G"
      },
      "source": [
        "#### Functions involving the summarization of the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRnYWYBjLwaS",
        "outputId": "4e46ed57-0207-40aa-a96b-7f7b1ffe22af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing yt_trans_summarize.py\n"
          ]
        }
      ],
      "source": [
        "# For text summarization\n",
        "%%writefile yt_trans_summarize.py\n",
        "from transformers import BartTokenizer, pipeline\n",
        "import transformers\n",
        "import pysbd\n",
        "import spacy\n",
        "import numpy as np\n",
        "import time\n",
        "import textwrap\n",
        "import assemblyai as aai\n",
        "\n",
        "from yt_vid_transcribe import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58_lAlVyK9nQ",
        "outputId": "4554b89a-bdeb-40ee-b0eb-e3a0baeab364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_trans_summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_trans_summarize.py\n",
        "\n",
        "# Function to remove incomplete sentences, which may occur either during transcription or in summary\n",
        "def remove_incomplete_sentences_spacy(paragraph):\n",
        "  # loading the spacy model\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  # Process the paragraph with spaCy\n",
        "  doc = nlp(paragraph)\n",
        "\n",
        "  # Extract complete sentences\n",
        "  complete_sentences = [sent.text for sent in doc.sents if sent.text.endswith(('.', '!', '?'))]\n",
        "\n",
        "  # Join the complete sentences back into a paragraph\n",
        "  cleaned_paragraph = ' '.join(complete_sentences)\n",
        "  return cleaned_paragraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zzFUQINSLMX",
        "outputId": "740d1924-56c6-4bf3-b1c3-e2fa026365ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_trans_summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_trans_summarize.py\n",
        "\n",
        "# Making a function to split the paragraphs into sentences\n",
        "# Get (list of sentences, list of lengths of each sentence) as result\n",
        "def get_sent_len(text_data):\n",
        "\n",
        "  # Using the sentence splitter\n",
        "  seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "\n",
        "  # Get sentences and their lengths in the lists\n",
        "  text_lst, lengths = [], []\n",
        "  for i in seg.segment(text_data):\n",
        "    text_lst.append(i)\n",
        "    lengths.append(len(i.split(\" \")))\n",
        "\n",
        "  return text_lst, lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kXFhOdGP3sm"
      },
      "source": [
        "```\n",
        "Since, we have transcripts which may be very long (>> 512 tokens), therefore to adhere to the token limit of the summarizer model, we\n",
        "split the long-text into chunks, which are then sent to the summarizer model, which summarizes those chunks\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF9_KM_YK9qs",
        "outputId": "da1dbfd2-e67d-402a-cfd1-1e95ad242366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_trans_summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_trans_summarize.py\n",
        "\n",
        "# Making a function to split the text into chunks\n",
        "def split_text_into_pieces(text, tokenizer, max_chunk_tokens=900, overlapPercent=10):\n",
        "\n",
        "  # Get sentences and their lengths after splitting the paragraph\n",
        "  text_lst, lengths = get_sent_len(text)\n",
        "\n",
        "  # Get percentile limit lenght of the sentences\n",
        "  # we will remove all sentences whose lengths are less than this limit\n",
        "  sent_len_limit = int(np.percentile(lengths, 20))\n",
        "\n",
        "  # Split the tokens into chunks of size max_tokens\n",
        "  pieces = []\n",
        "  temp_sentence = []\n",
        "  temp_length = 0\n",
        "  for data in text_lst:\n",
        "    # Get number of tokens in the data\n",
        "    sent_len = len(tokenizer.tokenize(data))\n",
        "\n",
        "    # If number of tokens < percentile limit, we won't consider them\n",
        "    if sent_len < sent_len_limit: continue\n",
        "\n",
        "    if temp_length <= max_chunk_tokens:\n",
        "      temp_sentence.append(data)\n",
        "      temp_length += sent_len\n",
        "    else:\n",
        "      pieces.append(\" \".join(temp_sentence))\n",
        "      temp_sentence = temp_sentence[-2:]\n",
        "      temp_length = 0\n",
        "\n",
        "  pieces.append(\" \".join(temp_sentence))\n",
        "  return pieces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15FX-yjuJnpH",
        "outputId": "61e5d4ca-3971-4031-a892-44b63b920eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_trans_summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_trans_summarize.py\n",
        "\n",
        "# Making a function to summarize using the model\n",
        "def summarize(text, summarizer, tokenizer, maxlength_percent = 0.75, minlength_percent = 0.45):\n",
        "\n",
        "  # Get number of tokens from the text\n",
        "  token_count = len(tokenizer.tokenize(text))\n",
        "  max_length = int(maxlength_percent * token_count)\n",
        "  min_length = int(minlength_percent * token_count)\n",
        "\n",
        "  # Get the summary using the model\n",
        "  summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "  return summary[0]['summary_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3GUBVn6TWHa"
      },
      "source": [
        "```\n",
        "It is seen that summarization models, in general, while summarizing large text (nearing their token limit), the quality of the summary\n",
        "diminishes very much.\n",
        "\n",
        "Also, since we are working with very long text data, the summarizer models don't have the ability to summarize them.\n",
        "\n",
        "Below is an recursive algorithm to summarize the long text data to an appropriate needed summary length\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMCwOS3fK9lF",
        "outputId": "f577612d-df7d-4543-ebb7-54b83e2e8416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_trans_summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_trans_summarize.py\n",
        "\n",
        "# Making a function to get summary of appropriate length for the long-text\n",
        "def recursive_summarize(text, summarizer, tokenizer,\n",
        "                        max_sum_length=200, maxChunkLength=None,\n",
        "                        recursionLevel = 0,\n",
        "                        maxlength_percent = 0.7, minlength_percent = 0.4):\n",
        "\n",
        "  recursionLevel=recursionLevel+1\n",
        "  print(\"\\n######### Recursion level: \", recursionLevel,\"########## \\n\")\n",
        "\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  expectedCountOfChunks = len(tokens)/max_sum_length\n",
        "  max_length=int(len(tokens)/expectedCountOfChunks)+2\n",
        "\n",
        "  print(f'Total count of tokens: {len(tokens)}')\n",
        "  print(f'Expected count of chunks: {expectedCountOfChunks}')\n",
        "  print(f\"Max length: {max_length}\")\n",
        "  print()\n",
        "\n",
        "  # Break the text into pieces of max_length\n",
        "  pieces = split_text_into_pieces(text, tokenizer,\n",
        "                                  max_chunk_tokens=maxChunkLength if maxChunkLength else max_length)\n",
        "\n",
        "  print(\"Number of pieces: \", len(pieces))\n",
        "  # Summarize each piece\n",
        "  summaries=[]\n",
        "  k=0\n",
        "  for k in range(0, len(pieces)):\n",
        "    # Get each piece / chunk of the text data\n",
        "    piece=pieces[k]\n",
        "\n",
        "    # Then summarize each chunk\n",
        "    summary = summarize(piece, summarizer,tokenizer,\n",
        "                        maxlength_percent = maxlength_percent,\n",
        "                        minlength_percent = minlength_percent)\n",
        "\n",
        "    # Remove incomplete sentences from the summary\n",
        "    summary = remove_incomplete_sentences_spacy(summary)\n",
        "\n",
        "    # Append them to the resultant summary list\n",
        "    summaries.append(summary)\n",
        "\n",
        "  # Concatente the summaries into a paragraph\n",
        "  concatenated_summary = ' '.join(summaries)\n",
        "  summary_tokens = tokenizer.tokenize(concatenated_summary)\n",
        "\n",
        "  upper_limit_len = int(max_sum_length * 1.25)\n",
        "  lower_limit_len = int(max_sum_length * 0.75)\n",
        "\n",
        "  # If the input recursionLevel==-1, then we don't require the need for recursion\n",
        "  # this is done by the user\n",
        "  if recursionLevel == 0:\n",
        "    end = time.time()\n",
        "    print(f'Total tokens in the summary: {len(summary_tokens)}')\n",
        "    return concatenated_summary\n",
        "\n",
        "  # If the length of the summary is greater than upper limit, we will again call the\n",
        "  # summarize function recursively, till we reach the require limit length\n",
        "  if len(summary_tokens) > upper_limit_len:\n",
        "      print(f\"Current output summary:\\n {textwrap.fill(concatenated_summary, 150)}\")\n",
        "      print(f'Total tokens in the summary: {len(summary_tokens)}')\n",
        "\n",
        "      # If the concatenated_summary is too long, repeat the process\n",
        "      print(\"\\n############# GOING RECURSIVE ##############\")\n",
        "      return recursive_summarize(concatenated_summary, summarizer, tokenizer,\n",
        "                                 max_sum_length=max_sum_length, recursionLevel=recursionLevel,\n",
        "                                 maxChunkLength = maxChunkLength)\n",
        "  else:\n",
        "    # Summarize the text last time\n",
        "    final_summary = summarize(concatenated_summary, summarizer, tokenizer,\n",
        "                              maxlength_percent = 0.85, minlength_percent = 0.45)\n",
        "\n",
        "    final_summary = remove_incomplete_sentences_spacy(final_summary)\n",
        "\n",
        "    final_summary_tokens = tokenizer.tokenize(final_summary)\n",
        "\n",
        "    flag = False\n",
        "    # If final summary text length becomes too short, then use the summary we had before the last summarization\n",
        "    if len(final_summary_tokens) < lower_limit_len:\n",
        "      final_summary = concatenated_summary\n",
        "      flag = True\n",
        "\n",
        "    print(f'Total tokens in the final summary: {len(summary_tokens) if flag else len(final_summary_tokens)}')\n",
        "\n",
        "    print(textwrap.fill(final_summary, 150))\n",
        "    return final_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSss8IjeWmed"
      },
      "source": [
        "```\n",
        "For summarization, two summarizer models are provided:\n",
        "* Model 1: Falconsai/text_summarization model\n",
        "           (Faster model, but outputs decent quality summaries)\n",
        "\n",
        "* Model 2: Charankumarpc/test-dialogue-summarization model\n",
        "           (this model takes time to summarize, but outputs good quality summaries)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H55llNB6PMnk",
        "outputId": "ee4b8ce1-7009-455c-de35-5ff7bd9636c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_trans_summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_trans_summarize.py\n",
        "\n",
        "# Making a function that gets the transcript, then summarizes the transcript as well\n",
        "# Similar to a overall functionality function\n",
        "def get_transcript_summary(video_data, summarizer_choice:int = 1,\n",
        "                           summary_len = 256,\n",
        "                           if_summary_req:bool = True):\n",
        "\n",
        "  # Now, create the tokenizer\n",
        "  # We will be using the \"facebook/bart-large-cnn\" tokenizer model\n",
        "  tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "  # Now, get the choice of summarizer\n",
        "  if summarizer_choice == 1:\n",
        "    summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "  elif summarizer_choice == 2:\n",
        "    summarizer = pipeline(\"summarization\", model=\"Charankumarpc/test-dialogue-summarization\")\n",
        "\n",
        "  # Now, start the timer for the summarization\n",
        "  start = time.time()\n",
        "\n",
        "  # Now, get the summary\n",
        "  summary = recursive_summarize(video_data['transcribed_text'],\n",
        "                                summarizer, tokenizer,\n",
        "                                max_sum_length=int(summary_len),\n",
        "                                maxChunkLength=256)\n",
        "\n",
        "\n",
        "  # End the timer\n",
        "  end = time.time()\n",
        "  time_taken = end-start\n",
        "  # print(f\"Total time to summarize the transcript (in seconds): {end-start}\")\n",
        "\n",
        "  # Add the summary to the result data\n",
        "  video_data['summary'] = summary\n",
        "  video_data['smry_time_taken'] = time_taken\n",
        "\n",
        "  return video_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZs3XHDgFeJg"
      },
      "source": [
        "#### Making function for topic detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGcUBq3cFhWA",
        "outputId": "14ade546-528e-4634-facd-7f94d21bc950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing yt_topic_detect.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile yt_topic_detect.py\n",
        "import assemblyai as aai\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHFmneaVJCtP",
        "outputId": "1f01d9a9-f2a5-4b3f-eabe-24b935df184d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to yt_topic_detect.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a yt_topic_detect.py\n",
        "\n",
        "# Making a function to get all topic discussed in the video\n",
        "def get_topics(video_data):\n",
        "  aai.settings.api_key = \"47cba2f273db4856b56fbf845c550c9f\"\n",
        "\n",
        "  # Configure the model and get the topic details\n",
        "  config = aai.TranscriptionConfig(iab_categories=True)\n",
        "  transcript = aai.Transcriber().transcribe(video_data['Filepath'], config)\n",
        "\n",
        "  # Make a dataframe of the topics discussed\n",
        "  topics_df = pd.DataFrame(transcript.iab_categories.summary.items())\n",
        "  topics_df.columns = ['topic','confidence']\n",
        "  topics_df[\"topic\"] = topics_df[\"topic\"].str.split(\">\")\n",
        "  expanded_topics = topics_df.topic.apply(pd.Series).add_prefix('topic_level_')\n",
        "\n",
        "  # Sort the dataframe according to the confidence\n",
        "  topics_df = topics_df.join(expanded_topics).drop('topic', axis=1).sort_values(['confidence'], ascending=False).fillna('')\n",
        "  return topics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ02WsZk8Jsy"
      },
      "source": [
        "### Creating a Streamlit dashboard / application to run all of the above things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77McW21Ye3ZO"
      },
      "source": [
        "**APP ARCHITECTURE**\n",
        "```\n",
        "#### First making the sidebar ####\n",
        "## This will consist of the input-box wherein the user inputs the yt URL\n",
        "## After entering the URL, we will ask whether the user requires the summary of the video\n",
        "## If yes, then we will use the get_transcript_summary() function with its arguement if_summary_req = True\n",
        "## and then make the user choose from 2 summarizer model options:\n",
        "#       model-1: Fast, but decent summary (takes less time)\n",
        "#       model-2: Little slower, but outputs good quality summary\n",
        "#  (Add a short note, just above choosing model option: \"With increase in length of video, the time to summarize will also increase\")\n",
        "## If no, then we'll use the get_transcript_summary() function with its arguement if_summary_req = False to get just the transcript and the metadata\n",
        "\n",
        "#### Second, making the main panel / screen of the app ####\n",
        "## After submitting the details from the sidebar, firstly if the URL is not valid, then we just show \"INPUT YT URL IS INVALID / NOT AVAILABLE\"\n",
        "## Else, if the URL is valid, the main panel is divided into 2 columns\n",
        "## 1st column consists of the metadata of the video (more details below) and 2nd column will be the image of the video\n",
        "## 1st column: Here, subheading = 'Details of the video'\n",
        "#     In this column, all the metadata of the youtube video will be shown using\n",
        "#     the get_vid_metadata() function, which returns us the metadata in this format (in a dictionary):\n",
        "#{\n",
        "#    \"Author Name\": author_name,\n",
        "#    \"Video Title\": video_title,\n",
        "#    \"Video ID\": video_id,\n",
        "#    \"Duration\": duration,\n",
        "#    \"Channel URL\": channel_url,\n",
        "#    \"Video Description\": video_description,\n",
        "#    \"Video Keywords\": video_keywords,\n",
        "#    \"Publish Date\": publish_date,\n",
        "#    \"Average Rating\": average_rating,\n",
        "#    \"Thumbnail URL\": thumbnail_url,\n",
        "#    \"Views\": views,\n",
        "#    \"Filepath\": file_path\n",
        "#}'''\n",
        "# These details are then shown in the 1st column\n",
        "# In the second column, the image gotten from \"Thumbnail URL\" from the output of the get_vid_metadata() function is shown\n",
        "# Also, the audio can also be played. This will be present in the column 2\n",
        "# Make the column 2 take lesser space as compared to column 1\n",
        "# The column 1 is in the left and the column 2 is in the right side\n",
        "## Then below them comes the video playback part, where we can play the video\n",
        "## Next, is the transcript container, which will have 2 tabs:\n",
        "#   First tab will have scrollable text with the transcript it it\n",
        "#   Second tab will have many containers containing segments of the transcript\n",
        "## Next, if the user requires a summary, then the summary will be shown\n",
        "## Next, the topics discussed in the video are being shown in a dataframe\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2EORKIrCp8s",
        "outputId": "1108019f-c438-4cea-d37d-0c6b1e2393c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "# Change settings so as to write the code on app.py\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import streamlit_scrollable_textbox as stx\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from yt_extract import *\n",
        "from yt_vid_transcribe import *\n",
        "from yt_trans_summarize import *\n",
        "from yt_topic_detect import *\n",
        "\n",
        "\n",
        "# Streamlit App\n",
        "def main():\n",
        "    ###########################\n",
        "    ### INITIAL PAGE CONFIG ###\n",
        "    ###########################\n",
        "    st.set_page_config(page_title='VidScript Summarizer',\n",
        "                       layout=\"wide\", initial_sidebar_state=\"expanded\")\n",
        "\n",
        "    # Title\n",
        "    st.title(\"VidScript Summarizer\")\n",
        "\n",
        "    ###########################\n",
        "    ####### SIDEBAR ###########\n",
        "    ###########################\n",
        "\n",
        "    st.sidebar.header(\"Enter details for the video 🤖\")\n",
        "    yt_url = st.sidebar.text_input(\"Enter YouTube URL 🎧\", value=\"None\", placeholder=\"Enter a URL...\")\n",
        "\n",
        "    if yt_url == \"None\":\n",
        "      return None\n",
        "\n",
        "    if_summary_req = st.sidebar.radio(\"Do you want a summary? 📝\", [\"Yes, I want it ✅\", \"No, there is no need ❌\"])\n",
        "\n",
        "    summary_details = {}\n",
        "\n",
        "    if if_summary_req == \"Yes, I want it ✅\":\n",
        "      # First, change if_summary_req to True\n",
        "      if_summary_req = True\n",
        "\n",
        "      # Making a subheading for summarizer\n",
        "      st.sidebar.subheader(\"Choose Summarizer Model\")\n",
        "      model_option = st.sidebar.selectbox(\n",
        "          \"Select Summarizer Model\",\n",
        "          [\"Model-1: Fast, but decent summary\", \"Model-2: Slower, but good quality summary\"],\n",
        "      )\n",
        "\n",
        "      # Display note about time increase for longer videos\n",
        "      st.sidebar.markdown(\"<small>(With an increase 🔼 in the length of the video, the time to summarize will also increase 🔼)</small>\",\n",
        "                          unsafe_allow_html=True)\n",
        "\n",
        "      # Process user's choice\n",
        "      if model_option == \"Model-1: Fast, but decent summary\":\n",
        "        summary_details['model_choice'] = 1\n",
        "      elif model_option == \"Model-2: Slower, but good quality summary\":\n",
        "        summary_details['model_choice'] = 2\n",
        "\n",
        "      # Get the summary size from the user\n",
        "      summary_size = st.sidebar.number_input(\"Enter summary size\", value=None, placeholder=\"Type a number...\")\n",
        "      summary_details['summary_size'] = summary_size\n",
        "\n",
        "    else:\n",
        "      # Since, summary not needed\n",
        "      if_summary_req = False\n",
        "\n",
        "    ##############################\n",
        "    ####### MAIN SCREEN  #########\n",
        "    ##############################\n",
        "\n",
        "    if st.sidebar.button(\"Submit\"):\n",
        "      # Get the video metadata first\n",
        "      video_metadata = get_vid_metadata(video_url = yt_url)\n",
        "\n",
        "      if video_metadata == -1:\n",
        "          st.error(\"The entered YouTube URL is invalid / not available. Please try again !!!\")\n",
        "      else:\n",
        "\n",
        "          # Make 2 columns\n",
        "          col1, col2 = st.columns([0.65, 0.35])\n",
        "\n",
        "          ####################################\n",
        "          ### SHOWING THE DETAILS OF THE VIDEO\n",
        "          ####################################\n",
        "\n",
        "          # 1st Column (left)\n",
        "          with col1:\n",
        "            with st.container(border=True):\n",
        "              st.subheader(\"Details of the video 📝\")\n",
        "\n",
        "              for key, value in video_metadata.items():\n",
        "                if key in [\"Filepath\", \"thumbnail_url\"]:\n",
        "                  continue\n",
        "                s = f\"<p><span style='font-size:18px; font-weight:bold;'>{key}</span>: {value}</p>\"\n",
        "                st.markdown(s, unsafe_allow_html=True)\n",
        "\n",
        "          #########################################\n",
        "          ### SHOWING THE VIDEO THUMBNAIL IMAGE ###\n",
        "          #########################################\n",
        "\n",
        "          # 2nd Column (right)\n",
        "          with col2:\n",
        "            with st.container(border=True):\n",
        "              st.subheader(\"Video Thumbnail 🖼️\")\n",
        "\n",
        "              # Adjust the image size to cover 35% of the main screen width\n",
        "              st.image(video_metadata[\"thumbnail_url\"],  use_column_width=True,\n",
        "                        caption = video_metadata['Video title'])\n",
        "\n",
        "              #####################################\n",
        "              ### PLAYING THE AUDIO IN COLUMN 2 ###\n",
        "              #####################################\n",
        "\n",
        "              audio_file = open(video_metadata['Filepath'], 'rb')\n",
        "              audio_bytes = audio_file.read()\n",
        "              st.audio(audio_bytes, format='audio/webm')\n",
        "\n",
        "          #######################################\n",
        "          ### PLAYTING THE VIDEO FROM THE URL ###\n",
        "          #######################################\n",
        "\n",
        "          with st.container(border=True):\n",
        "            st.subheader('Video Playback 📽️')\n",
        "            # Embed a youtube video\n",
        "            width = 70\n",
        "            side = max((100 - width) / 2, 0.01)\n",
        "            _, container, _ = st.columns([side, width, side])\n",
        "            container.video(data=yt_url)\n",
        "\n",
        "          ##############################################\n",
        "          ### Getting the transcript of the yt video ###\n",
        "          ##############################################\n",
        "\n",
        "          with st.container(border=True):\n",
        "            st.subheader('Video Transcript 📝')\n",
        "            st.markdown(\"Transcription may take some time (according to duration of the video) ⌛\")\n",
        "\n",
        "            # First get the transcription of the video\n",
        "            video_metadata = get_transcript(video_metadata, video_metadata['Filepath'])\n",
        "\n",
        "            # Make 2 tabs to show the transcript text in different ways\n",
        "            tab1, tab2 = st.tabs([\"Scrollable text\", \"Whole transcript text\"])\n",
        "\n",
        "            with tab1:\n",
        "              # Now, format the transcript for better representation on the scroll text box\n",
        "              # format: (start: start-time -> end: end-time)\n",
        "              #         (Text: text in the timestamp segment)\n",
        "              transcript = \"\"\n",
        "              for data in video_metadata['sentence_level_text']['segments']:\n",
        "                text = f\"\"\"start-time: {data['start']} --> end-time: {data['end']}\\nText: {textwrap.fill(data['text'], 120)}\\n\\n\"\"\"\n",
        "                transcript += text\n",
        "\n",
        "              stx.scrollableTextbox(text = transcript, height = 500)\n",
        "              st.write(f\"Time taken to transcribe the video: {video_metadata['trct_time']:.2f} seconds\")\n",
        "\n",
        "            with tab2:\n",
        "              for data in video_metadata['sentence_level_text']['segments']:\n",
        "                with st.container(border=True):\n",
        "                  text = f\"\"\"<p><b>start-time</b>: {data['start']} --> <b>end-time</b>: {data['end']}</p>\n",
        "                             <p><b>Text</b>: {textwrap.fill(data['text'], 120)}</p>\"\"\"\n",
        "                  st.markdown(text, unsafe_allow_html=True)\n",
        "              st.write(f\"Time taken to transcribe the video: {video_metadata['trct_time']:.2f} seconds\")\n",
        "\n",
        "          #########################################\n",
        "          ### GETTING SUMMARY IF USER HAS ASKED ###\n",
        "          #########################################\n",
        "\n",
        "          # Check if user has asked for summary\n",
        "          if if_summary_req:\n",
        "            # Now, display the summary\n",
        "            with st.container(border = True):\n",
        "              st.subheader('Summary of the video 📝')\n",
        "              st.markdown(\"Summarization may take some time (according to duration of the video) ⌛\")\n",
        "\n",
        "              video_metadata = get_transcript_summary(video_metadata,\n",
        "                                                      summarizer_choice = summary_details['model_choice'],\n",
        "                                                      summary_len = summary_details['summary_size']\n",
        "                                                      )\n",
        "              with st.container(border = True):\n",
        "                text = textwrap.fill(video_metadata['summary'], 120)\n",
        "                st.markdown(text, unsafe_allow_html=True)\n",
        "\n",
        "              st.write(f\"Time taken to summarize the video: {video_metadata['smry_time_taken']:.2f} seconds\")\n",
        "\n",
        "          #######################################################\n",
        "          ### GETTING THE TOPICS BEING DISCUSSED IN THE VIDEO ###\n",
        "          #######################################################\n",
        "\n",
        "          topics_df = get_topics(video_metadata)\n",
        "          with st.container(border = True):\n",
        "            st.subheader(\"Topics discussed in the video 📃\")\n",
        "            st.dataframe(topics_df, use_container_width=True)\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH8XMDctG0F_",
        "outputId": "d9cd0f19-511f-4c34-d247-25890aecae5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.117s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNLlybiXCWpl",
        "outputId": "1395a3d8-b442-43e2-c5a2-62c575e1c2b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mcode\u001b[0m EAUDITNOPJSON\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35maudit\u001b[0m No package.json found: Cannot audit a project without a package.json\n",
            "\u001b[0m\n",
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m A complete log of this run can be found in:\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m     /root/.npm/_logs/2024-01-28T10_43_40_330Z-debug.log\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!npm audit fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzu4PpLpG0Cm"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rfvUNwlAvvj",
        "outputId": "96c3ba3f-f096-48bd-d4e0-7306288850ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.105.42.50\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swGGVlk5Gz6w",
        "outputId": "02c75c02-c1b6-4634-e834-ccfe202a2903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.809s\n",
            "your url is: https://breezy-turtles-punch.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADkezg3LU4PZ"
      },
      "outputs": [],
      "source": [
        "https://www.youtube.com/watch?v=PxCCzJndruQ&pp=ygUSdGF2aXNoaSBrcmlzaCBuYWlr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PVqhzhc2CZKW",
        "outputId": "da4858ab-17d7-4e59-ce53-73edd0fc834a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'audioFkaxczyyavE_Fain 魂.mp4'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = \"audioFkaxczyyavE_Fain 魂.webm\"\n",
        "a[:-4] + \"mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqpGxS_zTCAd",
        "outputId": "378c37f3-6376-4af7-bb20-30cac0b5d486"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "https://www.youtube.com/watch?v=dlCl8jKgEHM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpFga3X7bU08"
      },
      "outputs": [],
      "source": [
        "https://youtu.be/4WGTlrR4WyE?list=RDMM"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ciQ6KcgfbrFU"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}